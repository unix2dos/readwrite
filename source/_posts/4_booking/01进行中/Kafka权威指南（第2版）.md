---
doc_type: weread-highlights-reviews
bookId: "3300044310"
author: 格温·沙皮拉 托德·帕利诺 拉吉尼·西瓦拉姆 克里特·佩蒂
cover: https://cdn.weread.qq.com/weread/cover/67/cpPlatform_cwu15DVxapa4MNDc5T5T1h/t7_cpPlatform_cwu15DVxapa4MNDc5T5T1h.jpg
reviewCount: 1
noteCount: 56
title: 《Kafka权威指南（第2版）》
date: 2023-09-16
---


## 第2章 安装Kafka


- 📌 Kafka用ZooKeeper来保存集群元数据和消费者信息， 
    - ⏱ 2023-09-16 12:18:39 

- 📌 为了保证高可用，ZooKeeper以集群（被称为群组）的方式运行。由于使用了再均衡算法，建议一个ZooKeeper集群应该包含奇数个节点（比如3个、5个等）。只有当群组中的大多数节点（也就是所谓的仲裁）处于可用状态时，ZooKeeper才能处理外部请求。也就是说，一个包含3个节点的群组允许1个节点失效，而一个包含5个节点的群组允许2个节点失效。 
    - ⏱ 2023-09-16 12:19:32 

- 📌 如何选择分区数量？在选择主题的分区数量时，需要考虑如下因素。● 主题需要达到多大的吞吐量？例如，是希望写入100 KBps还是1 GBps？● 从单个分区读取数据的最大吞吐量是多少？通常每个分区都会有一个消费者读取。（即使不使用消费者群组，消费者也必须读取分区中的所有消息。）如果你知道消费者将数据写入数据库的速率不会超过50 MBps，那么也就知道了从一个分区读取数据的吞吐量不需要超过50 MBps。 
    - ⏱ 2023-09-16 12:24:27 

- 📌 如果要向主题写入和从主题读取1 GBps的数据，并且每个消费者可以处理50 MBps的数据，那么至少需要20个分区。这样就可以让20个消费者同时读取这些分区，从而达到1 GBps的吞吐量。 
    - ⏱ 2023-09-16 12:25:26 

- 📌 Kafka通常根据配置的时间长短来决定数据可以被保留多久。我们使用log.retention.hours参数来配置时间，默认为168小时，也就是1周。 
    - ⏱ 2023-09-16 12:25:54 

- 📌 另一种数据保留策略是通过计算已保留的消息的字节总数来判断旧消息是否过期。这个字节总数阈值通过参数log.retention.bytes来指定，对应的是每一个分区。也就是说，如果一个主题包含8个分区，并且log.retention.bytes被设置为1 GB，那么这个主题最多可以保留8 GB的数据。 
    - ⏱ 2023-09-16 12:26:16 

- 📌 如果同时指定了log.retention.bytes和log.retention.ms（或另一个按时间保留的参数），那么只要任意一个条件得到满足，消息就会被删除。假设log.retention.ms被设置为86 400 000（也就是1天），log.retention.bytes被设置为1 000 000 000（也就是1 GB），如果消息字节总数不到一天就超过了1 GB，那么旧数据就会被删除。相反，如果消息字节总数小于1 GB，那么一天之后这些消息也会被删除，尽管分区的数据总量小于1 GB。为简单起见，建议只选择其中的一种保留策略，要么基于数据大小，要么基于时间，或者两种都不选择，以防发生意外的数据丢失。不过，对于复杂的场景，可以两种都使用。 
    - ⏱ 2023-09-16 12:27:31 

- 📌 如果主题的消息量不是很大，那么如何设置这个参数就变得尤为重要。如果一个主题每天只接收100 MB的消息，并且log.segment.bytes使用了默认设置，那么填满一个日志片段将需要10天。因为在日志片段被关闭之前消息是不会过期的，所以如果log.retention.ms被设为604 800 000（也就是1周），那么日志片段最多需要17天才会过期。这是因为关闭日志片段需要10天，而根据配置的过期时间，还需要再保留数据7天（要等到日志片段的最后一条消息过期才能将其删除）。 
    - ⏱ 2023-09-16 12:28:29 

- 📌 为了提升集群的数据持久性，可以将min.insync.replicas设置为2，确保至少有两个副本跟生产者保持“同步”。生产者需要配合将ack设置为all，这样就可以确保至少有两个副本（首领和另一个副本）确认写入成功，从而防止在以下情况下丢失数据：首领确认写入，然后发生停机，所有权被转移到一个副本，但这个副本没有写入成功。如果没有这些配置，则生产者会认为已经写入成功，但实际上消息丢失了。不过，这样做是有副作用的，因为需要额外的开销，所以效率会有所降低。因此，对于能够容忍偶尔消息丢失的高吞吐量集群，不建议修改这个参数的默认值。 
    - ⏱ 2023-09-16 12:30:16 

- 📌 Kafka本身不需要太多内存。一个每秒处理150 000条消息和每秒200 MB数据速率的broker，只需要5 GB堆内存，剩下的系统内存用于页面缓存。因为缓存了正在使用的日志片段，所以Kafka的性能得到了提升。这就是为什么不建议把Kafka同其他重要的应用程序部署在一起，因为它们需要共享页面缓存，从而会降低Kafka消费者的性能。 
    - ⏱ 2023-09-16 12:32:25 

- 📌 如果整个集群需要保留10 TB数据，每个broker可以存储2 TB，那么至少需要5个broker。另外，如果增加了复制系数，那么至少还需要多一倍的空间，具体取决于配置的复制系数是多少（第7章将介绍复制系数）。复制系数是指一个broker的数据需要被复制到多少个其他broker上。如果这个集群配置的复制系数是2，那么将需要至少10个broker。 
    - ⏱ 2023-09-16 12:34:09 

- 📌 随着时间的推移，Kafka对ZooKeeper的依赖在减少。在2.8.0版本中，Kafka做了一个完全无ZooKeeper的早期尝试，但还没有做好生产就绪的准备。一路走来，可以看到各个版本已经在逐步减少对ZooKeeper的依赖。例如，在旧版本Kafka中，除了broker，消费者也利用ZooKeeper来保存消费者群组的信息和已消费的主题的信息，并定期提交分区的偏移量（为了实现消费者群组内的故障转移）。在0.9.0.0版本中，消费者接口发生了变化，我们可以直接在Kafka中完成上述的这些任务。在每一个2.x版本中，我们都看到Kafka的一些路径移除了对ZooKeeper的依赖。现在，Kafka管理工具不再需要连到ZooKeeper，它们可以直接连到Kafka集群完成主题创建、动态配置变更等操作。同样，很多以前使用 --zookeeper选项的命令行工具现在改用了 --bootstrap-server。--zookeeper选项仍然可以使用，但已经处于被弃用的状态。当将来Kafka不再需要通过连接ZooKeeper进行主题的创建、管理或消费时，这个选项将被移除。 
    - ⏱ 2023-09-16 12:43:14 

- 📌 虽然不建议使用ZooKeeper来保存元数据，但消费者仍然可以选择是使用ZooKeeper还是Kafka来保存偏移量，还可以选择提交的时间间隔。如果消费者使用ZooKeeper来保存偏移量，那么每一个消费者会在每一个提交时间间隔内执行一次ZooKeeper写入操作。合理的偏移量提交时间间隔是1分钟，因为如果有消费者发生故障，那么消费者群组将在这段时间内读取到重复消息。 
    - ⏱ 2023-09-16 12:44:34 

- 📌 不管怎样，还是建议使用新版的Kafka消费者，并将偏移量提交到Kafka，消除对ZooKeeper的依赖。 
    - ⏱ 2023-09-16 12:44:50 

- 📌 如果可以的话，除了让多个Kafka集群共享一个ZooKeeper群组，不建议再把ZooKeeper共享给其他应用程序。Kafka对ZooKeeper的延迟和超时比较敏感，与ZooKeeper群组之间的一个通信中断都可能导致Kafka出现不可预测的行为。 
    - ⏱ 2023-09-16 12:45:24 
## 第3章 Kafka生产者——向Kafka写入数据


- 📌 除了内置的客户端，Kafka还提供了二进制连接协议。也就是说，可以通过直接向Kafka网络端口发送字节序列的方式从Kafka读取消息或向Kafka写入消息。有很多用C++、Python、Go等语言实现的Kafka客户端，它们都实现了Kafka的连接协议，使得Kafka的使用不仅限于Java。这些客户端不属于Apache Kafka项目 
    - ⏱ 2023-09-16 12:47:12 

- 📌 之前提到的信用卡事务处理系统不允许消息丢失或重复，可接受的最高延迟为500毫秒，对吞吐量要求较高——我们希望每秒可以处理100万条消息。另一种应用场景是保存网站的点击信息。在这个场景中，允许丢失少量消息或出现少量重复消息，延迟可以高一些，只要不影响用户体验就行。 
    - ⏱ 2023-09-16 12:47:59 

- 📌 不同的应用场景直接影响如何使用和配置生产者API。 
    - ⏱ 2023-09-16 12:48:08 

- 📌 先从创建一个ProducerRecord对象开始，其中需要包含目标主题和要发送的内容。另外，还可以指定键、分区、时间戳或标头。在发送ProducerRecord对象时，生产者需要先把键和值对象序列化成字节数组，这样才能在网络上传输。 
    - ⏱ 2023-09-16 12:49:31 

- 📌 如果没有显式地指定分区，那么数据将被传给分区器。分区器通常会基于ProducerRecord对象的键选择一个分区。选好分区以后，生产者就知道该往哪个主题和分区发送这条消息了。紧接着，该消息会被添加到一个消息批次里，这个批次里的所有消息都将被发送给同一个主题和分区。有一个独立的线程负责把这些消息批次发送给目标broker。 
    - ⏱ 2023-09-16 12:50:13 

- 📌 发送消息主要有以下3种方式。发送并忘记把消息发送给服务器，但并不关心它是否成功送达。大多数情况下，消息可以成功送达，因为Kafka是高可用的，而且生产者有自动尝试重发的机制。但是，如果发生了不可重试的错误或超时，那么消息将会丢失，应用程序将不会收到任何信息或异常。同步发送一般来说，生产者是异步的——我们调用send()方法发送消息，它会返回一个Future对象。可以调用get()方法等待Future完成，这样就可以在发送下一条消息之前知道当前消息是否发送成功。异步发送调用send()方法，并指定一个回调函数，当服务器返回响应时，这个函数会被触发。 
    - ⏱ 2023-09-16 12:52:30 

- 📌 如果可以接受潜在的数据丢失，那么就可以使用这种发送方式，但在生产环境中通常不会这么做。 
    - ⏱ 2023-09-16 12:53:58 

- 📌 因此，同步发送方式通常不会被用在生产环境中（但会经常被用在示例代码中）。 
    - ⏱ 2023-09-16 12:54:29 

- 📌 为了能够在异步发送消息时处理异常情况，生产者提供了回调机制。 
    - ⏱ 2023-09-16 12:55:02 

- 📌 acks指定了生产者在多少个分区副本收到消息的情况下才会认为消息写入成功。在默认情况下，Kafka会在首领副本收到消息后向客户端回应消息写入成功（Kafka 3.0预计会改变这个默认行为）。这个参数对写入消息的持久性有重大影响，对于不同的场景，使用默认值可能不是最好的选择。 
    - ⏱ 2023-09-16 12:56:14 

- 📌 acks=1如果acks=1，那么只要集群的首领副本收到消息，生产者就会收到消息成功写入的响应 
    - ⏱ 2023-09-16 12:56:40 

- 📌 你会发现，为acks设置的值越小，生产者发送消息的速度就越快。也就是说，我们通过牺牲可靠性来换取较低的生产者延迟。 
    - ⏱ 2023-09-16 12:57:14 

- 📌 不过，端到端延迟是指从消息生成到可供消费者读取的时间，这对3种配置来说都是一样的。这是因为为了保持一致性，在消息被写入所有同步副本之前，Kafka不允许消费者读取它们。 
    - ⏱ 2023-09-16 12:57:38 

- 📌 当生产者收到来自服务器的错误消息时，这个错误有可能是暂时的（例如，一个分区没有首领）。在这种情况下，retries参数可用于控制生产者在放弃发送并向客户端宣告失败之前可以重试多少次。在默认情况下，重试时间间隔是100毫秒，但可以通过retry.backoff.ms参数来控制重试时间间隔。 
    - ⏱ 2023-09-16 13:02:00 

- 📌 在默认情况下，只要有可用的发送者线程，生产者都会直接把批次发送出去，就算批次中只有一条消息。把linger.ms设置成比0大的数，可以让生产者在将批次发送给服务器之前等待一会儿，以使更多的消息加入批次中。虽然这样会增加一点儿延迟，但也极大地提升了吞吐量。这是因为一次性发送的消息越多，每条消息的开销就越小，如果启用了压缩，则计算量也更少了。 
    - ⏱ 2023-09-16 13:03:27 

- 📌 在默认情况下，生产者发送的消息是未经压缩的。这个参数可以被设置为snappy、gzip、lz4或zstd， 
    - ⏱ 2023-09-16 13:03:51 

- 📌 Kafka可以保证同一个分区中的消息是有序的。也就是说，如果生产者按照一定的顺序发送消息，那么broker会按照这个顺序把它们写入分区，消费者也会按照同样的顺序读取它们 
    - ⏱ 2023-09-16 13:25:05 

- 📌 如果要发送给Kafka的对象不是简单的字符串或整型，则既可以用通用的序列化框架（比如Avro、Thrift或Protobuf）来创建消息，也可以使用自定义序列化器。强烈建议使用通用的序列化框架 
    - ⏱ 2023-09-16 13:31:47 

- 📌 建议使用已有的序列化器和反序列化器，比如JSON、Avro、Thrift或Protobuf。 
    - ⏱ 2023-09-16 13:32:38 

- 📌 我们把所有在写入数据时需要用到的模式都保存在注册表中，然后在记录里引用模式的标识符。读取消息的应用程序使用标识符从注册表中拉取模式来反序列化记录。模式的保存和拉取分别由序列化器和反序列化器来完成。 
    - ⏱ 2023-09-16 13:35:31 

- 📌 ProducerRecord对象包含了主题名称、记录的键和值。Kafka消息就是一个个的键–值对，ProducerRecord对象可以只包含主题名称和值，键默认情况下是null。不过，大多数应用程序还是会用键来发送消息。键有两种用途：一是作为消息的附加信息与消息保存在一起，二是用来确定消息应该被写入主题的哪个分区 
    - ⏱ 2023-09-16 13:38:54 

- 📌 需像下面这样创建一个ProducerRecord即可。ProducerRecord<String, String> record =￼     new ProducerRecord<>("CustomerCountry", "Laboratory Equipment", "USA");如果要创建键为null的消息，那么不指定键就可以了。ProducerRecord<String, String> record =￼     new ProducerRecord<>("CustomerCountry", "USA"); ➊ 
    - ⏱ 2023-09-16 13:39:05 

- 📌 如果键为null，并且使用了默认的分区器，那么记录将被随机发送给主题的分区。分区器使用轮询调度(round-robin)算法将消息均衡地分布到各个分区中。从Kafka 2.4开始，在处理键为null的记录时，默认分区器使用的轮询调度算法具备了黏性。也就是说，在切换到下一个分区之前，它会将同一个批次的消息全部写入当前分区。这样就可以使用更少的请求发送相同数量的消息，既降低了延迟，又减少了broker占用CPU的时间。 
    - ⏱ 2023-09-16 13:39:41 

- 📌 如果键不为空且使用了默认的分区器，那么Kafka会对键进行哈希（使用Kafka自己的哈希算法，即使升级Java版本，哈希值也不会发生变化），然后根据哈希值把消息映射到特定的分区。这里的关键在于同一个键总是被映射到同一个分区， 
    - ⏱ 2023-09-16 13:41:40 
## 第4章 Kafka消费者——从Kafka读取数据


- 📌 Kafka消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者负责读取这个主题的部分消息。假设主题T1有4个分区，我们创建了消费者C1，它是群组G1中唯一的消费者，用于订阅主题T1。消费者C1将收到主题T1全部4个分区的消息， 
    - ⏱ 2023-09-16 13:45:27 

- 📌 如果在群组G1里新增一个消费者C2，那么每个消费者将接收到两个分区的消息。假设消费者C1接收分区0和分区2的消息，消费者C2接收分区1和分区3的消息， 
    - ⏱ 2023-09-16 13:45:33 

- 📌 如果群组G1有4个消费者，那么每个消费者将可以分配到一个分区 
    - ⏱ 2023-09-16 13:45:44 

- 📌 如果向群组里添加更多的消费者，以致超过了主题的分区数量，那么就会有一部分消费者处于空闲状态，不会接收到任何消息 
    - ⏱ 2023-09-16 13:45:54 

- 📌 向群组里添加消费者是横向扩展数据处理能力的主要方式。Kafka消费者经常需要执行一些高延迟的操作，比如把数据写到数据库或用数据做一些比较耗时的计算。在这些情况下，单个消费者无法跟上数据生成的速度，因此可以增加更多的消费者来分担负载，让每个消费者只处理部分分区的消息，这是横向扩展消费者的主要方式。于是，我们可以为主题创建大量的分区，当负载急剧增长时，可以加入更多的消费者。不过需要注意的是，不要让消费者的数量超过主题分区的数量，因为多余的消费者只会被闲置 
    - ⏱ 2023-09-16 15:42:38 
 

- 📌 消费者会向被指定为群组协调器的broker（不同消费者群组的协调器可能不同）发送心跳，以此来保持群组成员关系和对分区的所有权关系。心跳是由消费者的一个后台线程发送的，只要消费者能够以正常的时间间隔发送心跳，它就会被认为还“活着”。 
    - ⏱ 2023-09-16 15:45:51 

- 📌 如果消费者在足够长的一段时间内没有发送心跳，那么它的会话就将超时，群组协调器会认为它已经“死亡”，进而触发再均衡。如果一个消费者发生崩溃并停止读取消息，那么群组协调器就会在几秒内收不到心跳，它会认为消费者已经“死亡”，进而触发再均衡。在这几秒时间里，“死掉”的消费者不会读取分区里的消息。在关闭消费者后，协调器会立即触发一次再均衡，尽量降低处理延迟。 
    - ⏱ 2023-09-16 15:46:04 

- 📌 session.timeout.ms指定了消费者可以在多长时间内不与服务器发生交互而仍然被认为还“活着”，默认是10秒。如果消费者没有在session.timeout.ms指定的时间内发送心跳给群组协调器，则会被认为已“死亡”，协调器就会触发再均衡，把分区分配给群组里的其他消费者。 
    - ⏱ 2023-09-16 15:54:18 

- 📌 auto.offset.reset这个属性指定了消费者在读取一个没有偏移量或偏移量无效（因消费者长时间不在线，偏移量对应的记录已经过期并被删除）的分区时该做何处理。它的默认值是latest，意思是说，如果没有有效的偏移量，那么消费者将从最新的记录（在消费者启动之后写入Kafka的记录）开始读取。另一个值是earliest，意思是说，如果没有有效的偏移量，那么消费者将从起始位置开始读取记录。如果将auto.offset.reset设置为none，并试图用一个无效的偏移量来读取记录，则消费者将抛出异常。 
    - ⏱ 2023-09-16 15:55:54 

- 📌 那么消费者是如何提交偏移量的呢？消费者会向一个叫作 __consumer_offset的主题发送消息，消息里包含每个分区的偏移量。如果消费者一直处于运行状态，那么偏移量就没有什么实际作用。但是，如果消费者发生崩溃或有新的消费者加入群组，则会触发再均衡。再均衡完成之后，每个消费者可能会被分配新的分区，而不是之前读取的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的位置继续读取消息。 
    - ⏱ 2023-09-16 15:58:48 

- 📌 如果使用自动提交或不指定提交的偏移量，那么将默认提交poll()返回的最后一个位置之后的偏移量 
    - ⏱ 2023-09-16 16:00:53 

- 📌 假设我们使用默认的5秒提交时间间隔，并且消费者在最后一次提交偏移量之后3秒会发生崩溃。再均衡完成之后，接管分区的消费者将从最后一次提交的偏移量的位置开始读取消息。这个偏移量实际上落后了3秒，所以在这3秒内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，缩小可能导致重复消息的时间窗口，但无法完全避免。 
    - ⏱ 2023-09-16 16:02:12 

- 📌 虽然自动提交很方便，但是没有为避免开发者重复处理消息留有余地。 
    - ⏱ 2023-09-16 16:05:08 

- 📌 需要注意的是，commitSync()将会提交poll()返回的最新偏移量，所以，如果你在处理完所有记录之前就调用了commitSync()，那么一旦应用程序发生崩溃，就会有丢失消息的风险（消息已被提交但未被处理）。如果应用程序在处理记录时发生崩溃，但commitSync()还没有被调用，那么从最近批次的开始位置到发生再均衡时的所有消息都将被再次处理——这或许比丢失消息更好，或许更坏。 
    - ⏱ 2023-09-16 16:05:56 

- 📌 到目前为止，我们已经知道如何使用poll()从各个分区的最新偏移量位置读取消息，但有时候也需要从不同的偏移量位置读取消息。Kafka提供了一些方法，可以让poll()从不同的位置读取消息。如果你想从分区的起始位置读取所有的消息，或者直接跳到分区的末尾读取新消息，那么Kafka API分别提供了两个方法：seekToBeginning(Collection<Topic Partition> tp) 和seekToEnd(Collection<TopicPartition> tp)。 
    - ⏱ 2023-09-16 16:10:53 

# 读书笔记

## 第4章 Kafka消费者——从Kafka读取数据

### 划线评论
- 📌 如果新增一个只包含一个消费者的群组G2，那么这个消费者将接收到主题T1的所有消息，与群组G1之间互不影响。群组G2可以增加更多的消费者，每个消费者会读取若干个分区，就像群组G1里的消费者那样。作为整体来说，群组G2还是会收到所有消息，不管有没有其他群组存在  ^225207353-7LgMUG6Iy
    - 💭 每个消费组读自己的，互不影响
    - ⏱ 2023-09-16 15:44:01
   

# 本书评论
